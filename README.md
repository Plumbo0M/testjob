Ответы на вопросы
Что необходимо проверить перед запуском проекта в прод:
1)Убедится что кластер корректно настроен и готов обрабатывать внешние и внутренние подключения.
2)Убедится что на кластере достаточно ресурсов для развертывания проекта.
3)Проверить что для данного проекта установлены все необходимые зависимости.
4)Проверить сам helm чарт, убедится что все сервисы в values.yaml указаны корректно.
5)Проверить что каждый из сервисов из template может без проблем общаться с другими сервисами.
6)Проверить версии в репозитории, убедится что они могут работать с текущей версией helm и кластера k3s.
7)Развернуть чарт в тестовом namespace, проверить корректность создания подов, провести нагрузочные тесты и тесты на стабильность (к примеру удалить один из подов, система должна его пересоздать)
8)Проверить логи каждого пода, чтобы на момент запуска не было ошибок.
9)Убедится что есть документация для развертывания проекта.
10)Провести полное тестирования в тестовом namespace, по возможности привлечь qa.
11)Убедится что все заинтересованные лица в курсе о запуске на прод уже протестированного сервиса.
12)Продмать способы логирования и сбора информации для критически важных сервисов.
13)Выпускать на прод.

Теперь по сделанному.
Изначально у меня уже был свой кластер k3s с установленными helm, ingress. Поэтому я для эксперимента попробовал сразу установить указанный helm-chart.
Чарт установился и запустился без каких либо видимых ошибок.
После получения ответа о том, что точно есть что поправить решил проверить файлы более внимательно.
Во первых проверил values.yaml Нашел используемый репозиторий, изучил его.
Во вторых начал разбираться с templates. 
Что было сделано:
1)Некоторые значения initialDelaySeconds были увеличены, я использовал достаточно мощный кластер, но возможно если бы разворачивал кластер у себя на ноуте этих значений было мало и сервис мог не успеть стартануть.
2)Во всех сервисах добавил метку release. Просто прочитал о том что обычно так делать првильно.
3)Все порты, которые были указаны как статические переменные я вынес в values.yaml, чтобы проще было ими управлять и не лезть лишний раз в сами сервисы. В сервисах соответсвенно переделал, чтобы они читали это значение с valuees.
4)terminationGracePeriodSeconds увеличил с 5 до 30 секунд, чтобы у проесса точно было время на завершение.
5)Для редис в values указал конкретный образ redis:alpine@sha256:c35af3bbcef51a62c8bae5a9a563c6f1b60d7ebaea4cb5a3ccbcc157580ae098, так же добавил terminationGracePeriodSeconds: 30, для более корректной отработки в случае если под будет удален.
6)Немного увеличил ресурсы для Редиса, при тестовых нагрузках может хватить и меньших ресурсов, но при реальной нагрузке ему уже понадобиться больше.
7)Для cartservice,checkoutservice,currencyservice,emailservice,frontend так же добавил terminationGracePeriodSeconds.
8)В emailservice и frontend так же сделал порты управляемыми через values, readinessProbe и livenessProbe так же вынес в values.
9)Для loadgenerator вынес пересенную users в values
10)Для некоторых фалов увелчиил время на обработку запроса.

P.S Нашел одну ошибку, точнее предупреждение, удалось вывести, что гдето используется resources там где это не предусмотрено программой, полагаю в сервисе редиса. Попробую убрать протестировать, отпишусь.

Пока это все, но в теории можно сделать больше, например настроить нормальную репликацию, добавить полноценный мониторинг, расширить логирование. Возможно это не финальная версия.
